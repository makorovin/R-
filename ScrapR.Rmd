---
title: "Scrapping_with_R"
author: "Max"
date: '28 сентября 2017 г '
output: html_document
---
# Скачивание объявлений аренды склада с ЦИАН

Для решения собственных практически полезных задач анализа данных необходимо уметь выгружать данные с веб-страниц. Как правило, веб-страница - это документ, содержащий структурированные размеченные данные (с указанием на вид(текст, таблица, заголовок и т.п.)). Наиболее популярными инструментами создания структуры и разметки являются языки HTML и XML. HTML ([HyperText](https://ru.wikipedia.org/wiki/%D0%93%D0%B8%D0%BF%D0%B5%D1%80%D1%82%D0%B5%D0%BA%D1%81%D1%82) Markup Language) является наиболее распространенным инструментом представления веб-страниц, именно его интерпретирует браузер. Он позволяет структурировать данные, но его основное предназначение - визуальное отображение. XML (eXtensible Markup Language) предназначен для структурирования, хранения и передачи данных. HTML и XML очень похожи: данные представляются в виде дерева и могут быть преобразованы друг в друга, хотя имеют некоторые существенные [отличия](https://www.w3schools.com/xml/xml_whatis.asp).

![дерева XML](https://www.w3schools.com/xml/nodetree.gif) 

HTML/XML документы хранятся на [сервере](https://ru.wikipedia.org/wiki/%D0%A1%D0%B5%D1%80%D0%B2%D0%B5%D1%80_(%D0%B0%D0%BF%D0%BF%D0%B0%D1%80%D0%B0%D1%82%D0%BD%D0%BE%D0%B5_%D0%BE%D0%B1%D0%B5%D1%81%D0%BF%D0%B5%D1%87%D0%B5%D0%BD%D0%B8%D0%B5)). Для перехода на веб-страницу компьютер пользователя отправляет на сервер запрос (request), получает ответ (response) в виде документа HTML/XML (и т.п.), браузер пользователя считывает этот документ, и пользователь видит веб-страницу. 

![Общая логика работы запросов к серверу](https://webdev.dartlang.org/tutorials/images/client-server.png)

Для того, чтобы отправить запрос на сервер, получить ответ необходимо сначала установить между сервером и пользовательским компьютером двусторонее интернет-соединение для обмена данными. Для этого существуют разничные механизмы - протоколы передачи данных ([TCP/IP](https://www.objc.io/issues/10-syncing-data/ip-tcp-http/#http-hypertext-transfer-protocol)). После установки соединения возможна отправка запросов по протоколу [HTTP](https://www.objc.io/issues/10-syncing-data/ip-tcp-http/#http-hypertext-transfer-protocol)(HTTPS-safe HTTP). Для получения хранящегося на сервере документа, веб страницы используется метод GET протокола HTTP(S).

В R есть различные библиотеки, позволяющие отправлять на сервер запросы (соединение устанавливается автоматически), получать HTML/XML документы и работать с ними. Самыми удобными и распространенными являются пакеты httr - для работы с запросами и rvest - для чтения HTML.

## Импорт библиотек
Подключим указанные библиотеки, а также пакет для работы с данными tidyr. При необходимости используйте install.packages("package") для установки пакетов. 
```{r}
library(httr)
library(rvest)
library(tidyr)
```

## Анализ структуры страницы и получение части объявлений
### Анализ структуры ссылки на объявления

Посмотрим на адрес складских объявлений ЦИАН.

```{r}
url <- "https://www.cian.ru/cat.php?deal_type=rent&engine_version=2&offer_type=offices&office_type%5B0%5D=3&p=2&region=1"
```
URL (Uniform Resource Locator) состоит из нескольких частей:
* https - используемый протокол передачи данных
* www.cian.ru - полное доменное имя сайта - набора веб-страниц
  + www - префикс или домен 3-го уровня, обозначает принадлежность к World Wide Web
  + cian - имя сайта, домен 2-го уровня
  + ru - домен 1-го уровня, обозначающий принадлежность к русской части Интернет
* /cat.php - путь к файлу cat.php в файловом хранилище сервера
* ?deal_type=rent&engine_version=2... - параметры запроса, отделенные от основного адреса знаком ?  и разделенные между собой знаком &

Для обращения к сайту из R важно понимать структуру адреса сайта и использовать правильные параметры, чтобы правильно формировать запросы и получать нужную информацию. Часто бывает, что если сайт содержит несколько однотипных страниц под различными номерами, то адрес первой страницы отличается. Например, на сайте ЦИАН первая страница с объявлениями об аренде складов имеет адрес:https://www.cian.ru/snyat-sklad/, в то время как другие страницы имеют адрес, подобный указанному выше. Несмотря на это, первая страница может быть получена по адресу, аналогичному другим, с параметром p=1. В адресе выше нас интересует параметр p=2 - номер страницы, именно его следует менять при скачивании данных с разных по номеру страниц. На других сайтах могут меняться и другие параметры, поэтому первым этапом в скачивании данных с сайта явлется анализ его структуры, механизма изменения параметров при переходе на другие страницы.

### Формирование адреса страницы и GET запроса

Запишем базовый адрес, к которому через разделитель & будем добавлять параметр "p" с различными значениями. Обратите внимание, что порядок следования параметров обычно значения не имеет.
Запишем также новый адрес путем конкатенации строки базового URL и динамической строки параметра. Такой способ получения нового адреса немного грубый, но универсальный. Более изящно использовать query = list(param1 = "value1", param2 = "value2") в качестве аргумента ф-ии GET(). query содержит список именованных значений параметров. При этом не обязательно передавать все параметры в query(см. далее)

```{r}
base_url <- "https://www.cian.ru/cat.php?deal_type=rent&engine_version=2&offer_type=offices&office_type%5B0%5D=3&region=1"
new_url <- paste(base_url,sprintf("page=%d",1),sep="&")
new_url
```

Попробуем отправить GET запрос на новый адрес, используя ф-ю GET из httr. Переменную назовем response, поскольку она уже будет содержать ответ сервера.

```{r}
# Отправим get-запрос на сервер 
http_response <- GET(new_url)
# или, что то же самое
http_response <- GET(base_url,query = list(p="1"))
# Посмотрим на формат ответа
http_response

```

Видно, что ответ сервера имеет определенную структуру. Нас интересует код ответа (Status) и Content-Type или соответствующий заголовок документа <!DOCTYPE html>, содержащий информацию о формате полученного документа и его кодировке(utf-8). 200 - код успешного ответа. Коды, начинающиеся с цифры 4 будут свидетельствовать об ошибке на стороне клиента (404 page not found, 400 - bad request). В ответ от сервера мы получили html-документ. Считать его можно функциями content() или read_html(). Обе функции преобразуют HTML в xml_document или дерево XML.


```{r}
# Для чтения содержимого документа используем ф-ю content(), она преобразует html в xml_document
xml_response <- content(http_response)
# Посмотрим, как выглядит ответ
xml_response
# То же ф-ей read_html
read_html(http_response)
```
Видно, что документ в корне разделяется на head и body - элементы. <head> и <body> - теги соответствующих элементов. Элементы могут иметь атрибуты, текстовое значение и содержать другие элементы. Более подробно об XML можно посмотреть [здесь](https://www.w3schools.com/xml/xml_tree.asp)

Изучение структуры документа следует производить "из-под" браузера - открыть в браузере страницу, кликнуть правой кнопкой по свободному месту -> "Посмотреть код"(Chrome)/"Исследовать элемент"(Mozila Firefox). Должен открыться интерактивный инструмент DevTools. Далее нажатием Ctrl+Shift+C можно перейти в режим исследования, позволяющем видеть кодовое представление и местонахождение элемента в дереве. Этот режим значительно упрощает анализ структуры страницы и позволяет быстро находить идентификаторы элементов с необходимыми данными.

![DevTools for cian]("cian_devtools_screen.JPG")

Из всего документа нам нужны только объявления. Если аккуратно навести в режиме DevTools на объявление, можно увидеть следующий код <div class="offer-container--38nzf"><div, другие объявления также являются элементами div с атрибутом class, значение которого -  offer-container--38nzf. Если внимательно изучить элементы объявлений, можно заметить, что первые три элемента в значении атрибута содержат и другую строку через пробел: class="offer-container--38nzf offer-container--top3--3QuJo". Для получения всех объявлений на странице следует использовать первое значение атрибута class, передавая его в XPath. Xpath - язык запросов к XML-дереву. Любой элемент в документе xml можно получить, написав правильный запрос.  Современные браузеры поддерживают извлечение XPath выбранного элемента в буфер обмена (копирование), и можно заметить, что путь к разным объявлениям отличается только их порядковым номером в конце: 

//*[@id="frontend-serp"]/div/div[5]/div[1] - XPath первого объявления

//*[@id="frontend-serp"]/div/div[5]/div[2] - XPath второго объявления

//*[@id="frontend-serp"]/div/div[5]/div[8]  - XPath восьмого объявления

Это можно использовать, если:
* известно количество подобных элементов на странице 
* это количество постоянно на других страницах (что требует дополнительных изысканий), ведь нам интересно скачать все объявления
* страница имеет хорошую структуру, и на ней не будет элементов- не объявлений, имеющих путь, подобный тому, который имеют объявления. В нашем случае это так, однако можно считать это везением

Поэтому продемонстрируем также использование значения атрибута class для поиска элементов - объявлений.

Запишем общее для всех элементов-объявлений значение атрибута class
```{r}
ad_class_name <- "offer-container--38nzf"
```
Для поиска элементов xml в rvest есть функция xml_find_all.На вход она принимает xml_document или xml_element, а также строку запроса XPath. Для написания таких запросов следует ознакомиться с [документацией](https://www.w3schools.com/xml/xml_xpath.asp).

```{r}

```


```{r}
headers_class_name <- "header-link--22fgv"
description_class_name <- "description--20a7r description--bucket2--1thVs"
metro_class_name <- "underground-header--A7XgS"


# Используем xmlPath для получения списка объявлений //* - все элементы, [содержащие атрибут (@) class со значением имени класса объявления]
ads <- xml_find_all(xml_resp, sprintf("//*[contains(@class, '%s')]",ad_class_name))
# Посмотрим на полученный список
ads
# Посмотрим на первый элемент - xml_nodeset - объявление
ads[1]

# Для извлечения заголовка из первого объявления можно снова использовать xmlPath
ad_header <- xml_find_first(ads[1],sprintf("descendant::*[contains(@class,'%s')]",headers_class_name))
ad_header
xml_text(ad_header)
# Получим список заголовков всех объявлений на странице
# Можно искать заголовки в исходном документе, но это небезопасно,
# т.к. в документе могут содержаться элементы с таким же именем class (такое встречается часто),
# не относящиеся к объявлениям. Поэтому заголовки следует
# извлекать из уже добытых элементов объявлений

# Можно выбрать другой путь и использовать css
html_node(ads[[1]],".header-link--22fgv")

html_text(html_node(ads[[1]],".header-link--22fgv"))

html_node()

xml_find_first(ads,sprintf("descendant::*[contains(@class,'%s')]",headers_class_name))
html_node(ads,".header-link--22fgv")

get_pages_content <- function(page_nums){
  lapply(page_nums, function(page_n){
    GET(paste(base_url,sprintf("page=%d",page_n),sep="&")) %>% content()
  })
}

parse_page_ads <- function(page) {
  xml_find_all(page, sprintf("//*[contains(@class, '%s')]",ad_class_name))
}


parse_many_ads_by_class_name <- function(ads,class_name) {
  parse_ad_info_by_class_name <- function(ad, class_name) {
  xml_find_first(ad,sprintf("descendant::*[contains(@class,'%s')]",class_name))
  }
  lapply(ads,function(ad){
    parse_ad_info_by_class_name(ad,headers_class_name)
    })
}

pages <- get_pages_content(1:3)
pages

ads <- lapply(pages,parse_page_ads) %>% unlist(., recursive=FALSE)
ads[1:3]

headers <- parse_many_ads_by_class_name(ads,headers_class_name)
headers[1:3]


descriptions <- parse_many_ads_by_class_name(ads,description_class_name)
descriptions[1:3]

links_text <- lapply(headers, function(h){xml_attr(h,"href")}) %>% unlist()
links_text

headers_text <- lapply(headers, xml_text) %>% unlist()
head(headers_text,3)
tail(headers_text,3)
descriptions_text <- lapply(descriptions, xml_text) %>% unlist()
(df <- data.frame(header = headers_text,link = links_text, description = descriptions_text))
colnames(df) <- c("header","link","description")
colnames(df)
df[1:3,1]
```





