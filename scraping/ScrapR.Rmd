---
title: "Scrapping_with_R"
author: "Max"
date: '28 сентября 2017 г '
output: html_document
---
# Скачивание объявлений аренды склада с ЦИАН

Для решения собственных практически полезных задач анализа данных необходимо уметь выгружать данные с веб-страниц. Как правило, веб-страница - это документ, содержащий структурированные размеченные данные (с указанием на вид(текст, таблица, заголовок и т.п.)). Наиболее популярными инструментами создания структуры и разметки являются языки HTML и XML. HTML ([HyperText](https://ru.wikipedia.org/wiki/%D0%93%D0%B8%D0%BF%D0%B5%D1%80%D1%82%D0%B5%D0%BA%D1%81%D1%82) Markup Language) является наиболее распространенным инструментом представления веб-страниц, именно его интерпретирует браузер. Он позволяет структурировать данные, но его основное предназначение - визуальное отображение. XML (eXtensible Markup Language) предназначен для структурирования, хранения и передачи данных. HTML и XML очень похожи: данные представляются в виде дерева и могут быть преобразованы друг в друга, хотя имеют некоторые существенные [отличия](https://www.w3schools.com/xml/xml_whatis.asp).

![дерева XML](https://www.w3schools.com/xml/nodetree.gif) 

HTML/XML документы хранятся на [сервере](https://ru.wikipedia.org/wiki/%D0%A1%D0%B5%D1%80%D0%B2%D0%B5%D1%80_(%D0%B0%D0%BF%D0%BF%D0%B0%D1%80%D0%B0%D1%82%D0%BD%D0%BE%D0%B5_%D0%BE%D0%B1%D0%B5%D1%81%D0%BF%D0%B5%D1%87%D0%B5%D0%BD%D0%B8%D0%B5)). Для перехода на веб-страницу компьютер пользователя отправляет на сервер запрос (request), получает ответ (response) в виде документа HTML/XML (и т.п.), браузер пользователя считывает этот документ, и пользователь видит веб-страницу. 

![Общая логика работы запросов к серверу](https://webdev.dartlang.org/tutorials/images/client-server.png)

Для того, чтобы отправить запрос на сервер, получить ответ необходимо сначала установить между сервером и пользовательским компьютером двусторонее интернет-соединение для обмена данными. Для этого существуют разничные механизмы - протоколы передачи данных ([TCP/IP](https://www.objc.io/issues/10-syncing-data/ip-tcp-http/#http-hypertext-transfer-protocol)). После установки соединения возможна отправка запросов по протоколу [HTTP](https://www.objc.io/issues/10-syncing-data/ip-tcp-http/#http-hypertext-transfer-protocol)(HTTPS-safe HTTP). Для получения хранящегося на сервере документа, веб страницы используется метод GET протокола HTTP(S).

В R есть различные библиотеки, позволяющие отправлять на сервер запросы (соединение устанавливается автоматически), получать HTML/XML документы и работать с ними. Самыми удобными и распространенными являются пакеты httr - для работы с запросами и rvest - для чтения HTML.

## Импорт библиотек
Подключим указанные библиотеки, а также пакет для работы с данными tidyr. При необходимости используйте install.packages("package") для установки пакетов. 
```{r}
library(httr)
library(rvest)
library(tidyr)
```

## Анализ структуры страницы и получение части объявлений
### Анализ структуры ссылки на объявления

Посмотрим на адрес складских объявлений ЦИАН.

```{r}
url <- "https://www.cian.ru/cat.php?deal_type=rent&engine_version=2&offer_type=offices&office_type%5B0%5D=3&p=2&region=1"
```
URL (Uniform Resource Locator) состоит из нескольких частей:
* https - используемый протокол передачи данных
* www.cian.ru - полное доменное имя сайта - набора веб-страниц
  + www - префикс или домен 3-го уровня, обозначает принадлежность к World Wide Web
  + cian - имя сайта, домен 2-го уровня
  + ru - домен 1-го уровня, обозначающий принадлежность к русской части Интернет
* /cat.php - путь к файлу cat.php в файловом хранилище сервера
* ?deal_type=rent&engine_version=2... - параметры запроса, отделенные от основного адреса знаком ?  и разделенные между собой знаком &

Для обращения к сайту из R важно понимать структуру адреса сайта и использовать правильные параметры, чтобы правильно формировать запросы и получать нужную информацию. Часто бывает, что если сайт содержит несколько однотипных страниц под различными номерами, то адрес первой страницы отличается. Например, на сайте ЦИАН первая страница с объявлениями об аренде складов имеет адрес:https://www.cian.ru/snyat-sklad/, в то время как другие страницы имеют адрес, подобный указанному выше. Несмотря на это, первая страница может быть получена по адресу, аналогичному другим, с параметром p=1. В адресе выше нас интересует параметр p=2 - номер страницы, именно его следует менять при скачивании данных с разных по номеру страниц. На других сайтах могут меняться и другие параметры, поэтому первым этапом в скачивании данных с сайта явлется анализ его структуры, механизма изменения параметров при переходе на другие страницы.

### Формирование адреса страницы и GET запроса

Запишем базовый адрес, к которому через разделитель & будем добавлять параметр "p" с различными значениями. Обратите внимание, что порядок следования параметров обычно значения не имеет.
Запишем также новый адрес путем конкатенации строки базового URL и динамической строки параметра. Такой способ получения нового адреса немного грубый, но универсальный. Более изящно использовать query = list(param1 = "value1", param2 = "value2") в качестве аргумента ф-ии GET(). query содержит список именованных значений параметров. При этом не обязательно передавать все параметры в query(см. далее)

```{r}
base_url <- "https://www.cian.ru/cat.php?deal_type=rent&engine_version=2&offer_type=offices&office_type%5B0%5D=3&region=1"
new_url <- paste(base_url,sprintf("page=%d",1),sep="&")
new_url
```

Попробуем отправить GET запрос на новый адрес, используя ф-ю GET из httr. Переменную назовем response, поскольку она уже будет содержать ответ сервера.

```{r}
# Отправим get-запрос на сервер 
http_response <- GET(new_url)
# или, что то же самое
http_response <- GET(base_url,query = list(p="1"))
# Посмотрим на формат ответа
http_response

```

Видно, что ответ сервера имеет определенную структуру. Нас интересует код ответа (Status) и Content-Type или соответствующий заголовок документа <!DOCTYPE html>, содержащий информацию о формате полученного документа и его кодировке(utf-8). 200 - код успешного ответа. Коды, начинающиеся с цифры 4 будут свидетельствовать об ошибке на стороне клиента (404 page not found, 400 - bad request). В ответ от сервера мы получили html-документ. Считать его можно функциями content() или read_html(). Обе функции преобразуют HTML в xml_document или дерево XML.


```{r}
# Для чтения содержимого документа используем ф-ю content(), она преобразует html в xml_document
xml_response <- content(http_response)
# Посмотрим, как выглядит ответ
xml_response
# То же ф-ей read_html
read_html(http_response)
```
Видно, что документ в корне разделяется на head и body - элементы. <head> и <body> - теги соответствующих элементов. Элементы могут иметь атрибуты, текстовое значение и содержать другие элементы. Более подробно об XML можно посмотреть [здесь](https://www.w3schools.com/xml/xml_tree.asp)

Изучение структуры документа следует производить "из-под" браузера - открыть в браузере страницу, кликнуть правой кнопкой по свободному месту -> "Посмотреть код"(Chrome)/"Исследовать элемент"(Mozila Firefox). Должен открыться интерактивный инструмент DevTools. Далее нажатием Ctrl+Shift+C можно перейти в режим исследования, позволяющем видеть кодовое представление и местонахождение элемента в дереве. Этот режим значительно упрощает анализ структуры страницы и позволяет быстро находить идентификаторы элементов с необходимыми данными.

![DevTools for cian](cian_devtools_screen.JPG)

Из всего документа нам нужны только объявления. Если аккуратно навести в режиме DevTools на объявление, можно увидеть следующий код
```{html}
<div class="offer-container--38nzf"><div
```
Другие объявления также являются элементами div с атрибутом class, значение которого -  offer-container--38nzf. Если внимательно изучить элементы объявлений, можно заметить, что первые три элемента в значении атрибута содержат и другую строку через пробел: class="offer-container--38nzf offer-container--top3--3QuJo". Для получения всех объявлений на странице следует использовать первое значение атрибута class, передавая его в XPath. Xpath - язык запросов к XML-дереву. Любой элемент в документе xml можно получить, написав правильный запрос.  Современные браузеры поддерживают извлечение XPath выбранного элемента в буфер обмена (копирование), и можно заметить, что путь к разным объявлениям отличается только их порядковым номером в конце: 

//*[@id="frontend-serp"]/div/div[5]/div[1] - XPath первого объявления

//*[@id="frontend-serp"]/div/div[5]/div[2] - XPath второго объявления

//*[@id="frontend-serp"]/div/div[5]/div[8]  - XPath восьмого объявления

Это можно использовать, если:
* известно количество подобных элементов на странице 
* это количество постоянно на других страницах (что требует дополнительных изысканий), ведь нам интересно скачать все объявления
* страница имеет хорошую структуру, и на ней не будет элементов- не объявлений, имеющих путь, подобный тому, который имеют объявления. В нашем случае это так, однако можно считать это везением

Поэтому продемонстрируем также использование значения атрибута class для поиска элементов - объявлений.

Запишем общее для всех элементов-объявлений значение атрибута class
```{r}
ad_class_name <- "offer-container--38nzf"
```
Для поиска элементов xml в rvest есть функция xml_find_all. На вход она принимает xml_document или xml_element, а также строку запроса XPath. Для написания таких запросов следует ознакомиться с [документацией](https://www.w3schools.com/xml/xml_xpath.asp).

```{r}
# Используем xmlPath для получения списка объявлений //* - все элементы, [содержащие атрибут (@) class со значением имени класса объявления]
ads <- xml_find_all(xml_response, sprintf("//*[contains(@class, '%s')]",ad_class_name))
# Посмотрим на полученный список
ads
# Посмотрим на первый элемент - xml_nodeset - объявление
ads[1]
```
То же можно сделать, используя [css selectors](https://www.w3schools.com/cssref/css_selectors.asp) и ф-ю html_node(xml_doc, css_selector). Для поиска по имени атрибута class в css нужно добавить перед именем "."
```{r}
paste(".",ad_class_name,sep="")
ads <- html_nodes(xml_response, paste(".",ad_class_name,sep=""))
# Посмотрим на полученный список
ads
```
Посмотрим на первый элемент - xml_nodeset - объявление
```{r}
ads[1]
```

Для получения текста из элементов можно использовать xml_text(xml_doc). Поосмотрим, что получится, если передать в ф-ю целое объявление

```{r}
xml_text(ads[1])
```
Похоже на то, что мы действительно выделили объявление, однако общий текст составлен последоватеьно из всех текстовых элементов внутри объявления. С таким текстом неудобно работать.

Объявление, как и документ, имеет свою структуру. Необходимые данные внутри него представлены в виде дерева и добыть их можно аналогичными способами. Запишем имена классов для элементов заголовка, описания и станции метро

```{r}
headers_class_name <- "header-link--22fgv"
description_class_name <- "description--20a7r"
metro_class_name <- "underground-header--A7XgS"

# Для извлечения заголовка из первого объявления можно снова использовать xmlPath
# Нам нужны все(*) потомки (descendant) [содержащие имя класса, 
# соответсвующее имени класса для заголовков]
ad_header <- xml_find_first(ads[1],sprintf("descendant::*[contains(@class,'%s')]",headers_class_name))
ad_header
```
Или снова используем css selector "." для class
```{r}
ad_header <- html_node(ads[1],paste(".",headers_class_name,sep=""))
ad_header
```
Или более общее, но менее безопасное в случае неуникальных или коротких имен [class~=имя_класса], class~= означает поиск элемента, атрибут class которого содержит имя_класса
```{r}
ad_header <- html_node(ads[1],sprintf("[class~=%s]",headers_class_name))
ad_header
```

Выделим текст заголовка
```{r}
xml_text(ad_header)
```
То, что нужно! Стоит отметить, что в данном случае можно найти элементы заголовков, описаний сразу в исходном документе, пропустив выделение элементов объявлений. Однако это работает только для хорошо размеченных страниц, и даже для них безопаснее сначала найти целые объявления, а уже внутри них искать нужную информацию. Это также позволяет обнаружить, в каком объявлении не указана информация, например, о станции метро

В R можно передать соответствующей ф-ии сразу все объявления
```{r}
# Получим список заголовков всех объявлений на странице
ads_headers <- html_node(ads,paste(".",headers_class_name,sep=""))
# или
ads_headers <- xml_find_all(ads,sprintf("descendant::*[contains(@class,'%s')]",headers_class_name))
print(paste("Длина списка заголовков соответствует длине списка объявлений:",if(length(ads_headers)==length(ads)) "да"  else "Нет"))
print(paste("Первый заголовок:",xml_text(ads_headers[1])))
```

Пришло время скачать объявления с нескольких страниц. Для этого напишем некоторые функции. В них будет использоваться xml_path. Попрактикуйтесь и попробуйте использовать альтернативные методы html_node и css для поиска элементов и query для запросов GET
```{r}
# Для получения xml_document путем отправки запроса GET
# по адресу страницы с объявлениями с различными номерами (page_nums)
get_pages_content <- function(page_nums){
  lapply(page_nums, function(page_n){
    GET(paste(base_url,sprintf("page=%d",page_n),sep="&")) %>% content()
  })
}
# Ф-я находит все объявления на странице
parse_page_ads <- function(page) {
  xml_find_all(page, sprintf("//*[contains(@class, '%s')]",ad_class_name))
}

# По заданному имени класса ф-я находит в объявлениях нужные элементы
parse_many_ads_by_class_name <- function(ads,class_name) {
  parse_ad_info_by_class_name <- function(ad, class_name) {
  xml_find_first(ad,sprintf("descendant::*[contains(@class,'%s')]",class_name))
  }
  lapply(ads,function(ad){
    parse_ad_info_by_class_name(ad,class_name)
    })
}

# Скачаем первые 3 страницы
pages <- get_pages_content(1:3)
pages
```
На страницах найдем все объявления. Ф-я lapply(iterable,FUN) вернет список списков объявлений, поэтому следует передать ее результат ф-ии unlist без рекурсии, т.е без разворачивания уже развернутых элементов
```{r}
ads <- lapply(pages,parse_page_ads) %>% unlist(., recursive=FALSE)
print(length(ads))
ads[1:3]
```
Теперь для всех объявлений на 3-х страницах найдем заголовки
```{r}
headers <- parse_many_ads_by_class_name(ads,headers_class_name)
headers[1]
headers_text <- lapply(headers, xml_text) %>% unlist()
headers_text[1:3]
```

Сделаем то же самое для текстовых описаний
```{r}
descriptions <- parse_many_ads_by_class_name(ads,description_class_name)
descriptions[1]
descriptions_text <- lapply(descriptions, xml_text) %>% unlist()
descriptions_text[1]
```
Для станций метро
```{r}
metro_stations <- parse_many_ads_by_class_name(ads,metro_class_name)
metro_stations[1]
metro_stations_text <- lapply(metro_stations, xml_text) %>% unlist()
metro_stations_text

```
Отлично, видно, что станция метро указана не для всех объявлений, при этом длина списка станций соответствует кол-ву объявлений. Мы также можем извлечь адреса, явно указанные классы складов, стоимость аренды м2 в год.

```{r}
adderss_class_name <- "address-path--12tl2"

addresses <- parse_many_ads_by_class_name(ads,adderss_class_name)
addresses[1]
addresses_text <- lapply(addresses, xml_text) %>% unlist()
addresses_text[1:3]
```
5 элементов соответствуют в данном случае 5-ти частям адреса

Аналогично для классов складов
```{r}
classType_class_name <- "header-classType--ECSQ1"

classTypes <- parse_many_ads_by_class_name(ads,classType_class_name)
classTypes[1]
classTypes_text <- lapply(classTypes, xml_text) %>% unlist()
classTypes_text
```
Большинство авторов объявлений не указали класс склада явно, но при чтении описаний выяснится, что класс указан именно в них.

Со стоимостью аренды м2 в год сложнее, поскольку ей не соответствует имякласса и какие-либо атрибуты - это просто комментарий в элементе div. Но можно выяснить, что эта информация находится на одном уровне с родителем элемента, который мы нашли для заголовков. Можно использовать эту информацию.
```{r}
xml_parent(xml_parent(headers[[1]])) %>% xml_children()
# или лучше
xml_siblings(xml_parent(headers[[1]]))
```
В первом случае мы перешли по дереву вверх 2 раза, получили дедушку(родителя родителя) исходного элемента, а затем нашли его детей. Звучит забавно, но фактически это так и следует из терминологии xml. Здесь мы получаем список детей, в который входит уже найденный нами родитель, поэтому это избыточно. Во втором случае мы сначала нашли родителя исходного элемента, а затем получили его бартьев-сестер (siblings). В последнем случае мы уже имеем искомый элемент. Для того, чтобы проделать это со всеми объявлениями стоит помнить, что эта информация может быть не указана. Можно было бы использовать проверку длины списка найденных братьев-сестер, но R уже обо всем позаботился и передача xml_nodeset(0) в ф-ю xml_text даст NA

```{r}
prices_per_year_m2 <- lapply(headers,function(header){xml_siblings(xml_parent(header))})
prices_per_year_m2_text <- lapply(prices_per_year_m2,xml_text) %>% unlist()
prices_per_year_m2_text[1:30]
```
Если внимательно изучить страницы с объявлениями можно заметить, что объявления, выделенные желтым, могут повторяться на разных страницах и меняться при обновлении одной страницы - все это в рекламных целях. Для нас это означает, что при загрузке данных с разных страниц могут присутствовать дубликаты, а при многократном обращении к одной странице часть объявлений будет меняться. Для удаления дубликатов нужен уникальный идентификатор. Им может служить ссылка на объявление или ее часть, id. Ссылки нам понадобятся, т.к у объявления есть собственная страница с указанием более подробной информации. Подобная ситуация и со многими другими сайтами.

При скачивании данных о множестве однотипных элементов, в данном случае объявлений, нужно с самого начала определить, могут ли встречаться дубликаты и понадобится ли уникальный идентификатор. На практике часто нужно обработать несколько десятков, реже сотен, бывает, что и тысячи страниц. Это занимает продолжительное время, и лучше избежать перезапуска этой процедуры либо путем скачивания сначала документов со всех страниц, либо путем обнаружения и записи уникального идентификатора элементов, если по каким-либо причинам скачать/хранить все документы веб-страниц затруднительно.

Ссылку на страницу объявления можно заметить среди атрибутов найденных нами заголовков.Извлечь ее из тела xml_element/xml_nodeset можно ф-ей xml_attr(xml_el,"имя_атрибута"). Ссылки имеют имя атрибута "href"

```{r}
links_text <- lapply(headers, function(h){xml_attr(h,"href")}) %>% unlist()
head(links_text,3)
tail(links_text,3)
```
Запишем данные в таблицу data.frame
```{r}
(df <- data.frame(header = headers_text,link = links_text, description = descriptions_text, address  = addresses_text, price_per_year_m2 = prices_per_year_m2_text, classType = classTypes_text))
str(df)
```

Выделим уникальные идентификаторы объявлений из ссылок. Нам повезло, дубликатов нет
```{r}
ids <- lapply(df$link, function(l){strsplit(as.character(l),"/")[[1]][6]} %>% as.integer) %>%     unlist()
df$id <- ids
str(df$id)
length(unique(df$id))
```
Теперь посмотрим на структуру страницы объявления 

![Ad_page_code_screen](Ad_page_code_screen.png)

Заметно, что необходимая нам информация, содержится в элементах с именами классов cf-comm-offer-detail__prop-name - для обозначения названия характеристики(напр. тип здания, этаж) и cf-comm-offer-detail__prop-val - для обращения к значению х-ки(нежилой фонд, 20 м2). Анализ таких элементов на других страницах позволяет понять, что имена классов не изменяются, однако множества указанных характеристик меняется, т.е для одного объявления указан этаж, но не указан тип здания, для другого объявления - наоборот. Поэтому следует:
* сначала скачать все документы страниц объявлений
* пройдя по страницам, найти все названия характеристик
* для каждой страницы и характеристики получить значение характеристики по соответствующему запросу css или xpath

Скачаем страницы всех ранее найденных объявлений. Довольно часто возникают следующие проблемы:
* при запросе GET R отправляет на сервер заголовки по умолчанию и выдает этим свою нечеловеческую природу. Это вызывает автоматическую переадресацию на страницу капчи (Captcha). Для обхода следует изучить, какие заголовки отправляет Ваш браузер. Это можно сделать на панели [Network](https://stackoverflow.com/questions/4423061/view-http-headers-in-google-chrome) в DevTools в Chrome. В GET заголовки можно передать в аргументах с помощью add_headers("h1_name"="h1_value","h2_name"="h2_value",...)
* R может генерировать запросы довольно часто (несколько раз в секунду). Сервер отслеживает IP адреса, отправляющие запросы слишком часто, и перенаправляет их на страницу капчи. Для обхода этого можно применять ф-ю Sys.sleep(time_in_seconds), которая останавливает выполнение кода на заданное кол-во секунд


```{r}
base_page_url <- "https://www.cian.ru/rent/commercial/"
ad_pages <- lapply(links_text, function(link) {
  page <- GET(link, add_headers("Accept"="*/*",
                      "Accept-Encoding"="gzip,deflate,br",
                      "Accept-Language"="ru-RU,ru;q=0.8,en-US;q=0.6,en;q=0.4",
                      "Connection"="keep-alive",
                      "Referer"="https://www.cian.ru/rent/commercial/161180454/",
                      "User-Agent"="Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36")) %>% content()
  Sys.sleep(0.5)
  page
  })
length(ad_pages)
xml_text(ad_pages[[84]])
```
Теперь найдем названия всех характеристик

```{r}
html_nodes(ad_pages[[1]],".cf-comm-offer-detail__prop-name")
characteristics <- lapply(ad_pages, function(ad_page){
  html_nodes(ad_page,".cf-comm-offer-detail__prop-name") %>% 
  lapply(., xml_text())
  
})
```


```{r}
library(ggplot2)
library(ggmap)
library(tm)
library(leaflet)
library(htmlwidgets)
```

```{r}
coords <- geocode(lapply(df$address,function(addr){as.character(addr)})%>%unlist(), output = "latlona")
str(coords)
df$latitude <- coords$lat
df$longitude <- coords$lon
str(df)
```

```{r}
coords_no_na <- drop_na(coords)
leaflet()%>%addTiles() %>% addCircles(lng=~lon,lat=~lat)
m <- leaflet() %>%
  addTiles() %>%  # Add default OpenStreetMap map tiles
  addMarkers(lng=174.768, lat=-36.852, popup="The birthplace of R")
m  # Print the map

```

